{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Any, Optional, Tuple, Literal, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import sigkernel\n",
    "import os\n",
    "import sys\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "import ksig\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kernels.abstract_base import TimeSeriesKernel, StaticKernel\n",
    "from kernels.static_kernels import LinearKernel, RBFKernel, PolyKernel\n",
    "from kernels.integral import StaticIntegralKernel\n",
    "from kernels.sig_pde import SigPDEKernel\n",
    "from kernels.sig_trunc import TruncSigKernel\n",
    "from kernels.gak import GlobalAlignmentKernel, sigma_gak\n",
    "from kernels.reservoir import ReservoirKernel\n",
    "\n",
    "from features.random_fourier import RBF_RandomFourierFeatures\n",
    "from features.random_sig_fourier import TRP_RFSF_Gaussian, TRP_RFSF_Linear\n",
    "from features.signature import sig, logsig\n",
    "from features.random_warping_series import RandomWarpingSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Volterra Reservoir Kernel ###\n",
    "#################################\n",
    "\n",
    "def naive_gram(X, Y, tau, gamma):\n",
    "    N, T, d = X.shape\n",
    "    N2, T, d = Y.shape\n",
    "    lin_ker = LinearKernel()\n",
    "    state_space_gram = lin_ker(X, Y) #shape N1 N2 T\n",
    "    prod = gamma**2 / (1 - tau**2 * state_space_gram)\n",
    "\n",
    "    gram = torch.zeros(N, N2, dtype=X.dtype, device=X.device)\n",
    "    for i in range(N):\n",
    "        for j in range(N2):\n",
    "            outer = 0\n",
    "            for t in range(T):\n",
    "                inner = 1\n",
    "                for s in range(t+1):\n",
    "                    inner *= prod[i, j, s]\n",
    "                outer += inner\n",
    "            gram[i, j] = 1 + outer\n",
    "    return gram\n",
    "\n",
    "\n",
    "def naive_gram2(X, Y, tau, gamma):\n",
    "    N, T, d = X.shape\n",
    "    N2, T, d = Y.shape\n",
    "    lin_ker = LinearKernel()\n",
    "    state_space_gram = lin_ker(X, Y) #shape N1 N2 T\n",
    "    prod = gamma**2 / (1 - tau**2 * state_space_gram)\n",
    "\n",
    "    gram = torch.zeros(N, N2, dtype=X.dtype, device=X.device)\n",
    "    gram = gram + 1/(1-gamma**2)\n",
    "    for t in range(T):\n",
    "        gram = 1 + prod[:,:,T-1-t] * gram\n",
    "    return gram\n",
    "\n",
    "\n",
    "\n",
    "def volterra_reservoir_kernel_test():\n",
    "    N = 3\n",
    "    N2 = 2\n",
    "    T = 70\n",
    "    d = 2\n",
    "    dtype = torch.float32\n",
    "    torch.manual_seed(3)\n",
    "    X = torch.randn(N, T, d, dtype=dtype).to(\"cuda\").detach()\n",
    "    Y = torch.randn(N2,T, d, dtype=dtype).to(\"cuda\").detach()\n",
    "    tau = 0.9 / torch.maximum(X.abs().max(), Y.abs().max())\n",
    "    print(\"tau\", tau)\n",
    "    gamma = 0.9\n",
    "\n",
    "    ker = ReservoirKernel(tau=tau, gamma=gamma)\n",
    "    gram = ker(X, Y)\n",
    "    print(\"gram\\n\", gram)\n",
    "    naive = naive_gram(X, Y, tau, gamma)\n",
    "    print(\"naive\\n\", naive)\n",
    "    naive2 = naive_gram2(X, Y, tau, gamma)\n",
    "    print(\"naive2\\n\", naive2)\n",
    "volterra_reservoir_kernel_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# ######### RWS #########\n",
    "# #######################\n",
    "\n",
    "def RWS_test():\n",
    "    # Setup and parameters\n",
    "    N = 3\n",
    "    N2 = 2\n",
    "    T = 70\n",
    "    d = 2\n",
    "    dtype = torch.float32\n",
    "    #torch.manual_seed(3)\n",
    "    X = torch.randn(N, T, d, dtype=dtype).to(\"cuda\").detach()\n",
    "    Y = torch.randn(N2,T, d, dtype=dtype).to(\"cuda\").detach()\n",
    "    n_features = 20\n",
    "    D_min=2\n",
    "    D_max=5\n",
    "    sigma=1\n",
    "\n",
    "    # RWS\n",
    "    rws = RandomWarpingSeries(n_features, D_min, D_max, sigma, LinearKernel(scale=1/d/T))\n",
    "    feat_X = rws(X)\n",
    "    feat_Y = rws(Y)\n",
    "    K_trp = 1 + feat_X @ feat_Y.T\n",
    "    print(feat_X)\n",
    "    print(feat_X.shape)\n",
    "    print(K_trp)\n",
    "\n",
    "RWS_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#### Lib signatory wrapper ####\n",
    "###############################\n",
    "\n",
    "def test_signatory():\n",
    "    N,T,d = 2, 30, 2\n",
    "    trunc_level = 3\n",
    "    X = torch.randn(N, T, d)\n",
    "    sigs = sig(X, trunc_level)\n",
    "    logsigs = logsig(X, trunc_level)\n",
    "    print(\"sigs\", sigs.shape, sigs)\n",
    "    print(\"logsigs\", logsigs.shape, logsigs)\n",
    "\n",
    "    # no batch case\n",
    "    sigs0 = sig(X[0], trunc_level)\n",
    "    logsigs0 = logsig(X[0], trunc_level)\n",
    "    print(\"sigs0\", sigs0.shape, sigs0)\n",
    "    print(\"logsigs0\", logsigs0.shape, logsigs0)\n",
    "    \n",
    "# test_signatory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#### Linear TRP-RFSF features  vs  Vanilla Sig kernel ####\n",
    "##########################################################\n",
    "\n",
    "def LINEAR_trp_vs_kernel():\n",
    "    #parameters\n",
    "    N = 3\n",
    "    N2 = 2\n",
    "    T = 20\n",
    "    d = 2\n",
    "    trunc_level = 5\n",
    "    n_features = d\n",
    "    dtype = torch.float32\n",
    "    #torch.manual_seed(3)\n",
    "    X = torch.randn(N, T, d, dtype=dtype).to(\"cuda\").detach() / np.sqrt(d)\n",
    "    Y = torch.randn(N2,T, d, dtype=dtype).to(\"cuda\").detach() / np.sqrt(d)\n",
    "\n",
    "    #exact sig kernel\n",
    "    sigker = TruncSigKernel(LinearKernel(), normalize=False, trunc_level=trunc_level, geo_order=1, max_batch=50000)\n",
    "    K = sigker(X,Y)\n",
    "    print(\"exact\\n\", K)\n",
    "\n",
    "    #trp\n",
    "    MC_iter = 10000\n",
    "    res = []\n",
    "    for i in tqdm(range(MC_iter)):\n",
    "        trp = TRP_RFSF_Linear(trunc_level, n_features, only_last=True)\n",
    "        feat_X = trp(X)\n",
    "        feat_Y = trp(Y)\n",
    "        K_trp = 1 + feat_X @ feat_Y.T\n",
    "        res.append(K_trp)\n",
    "    res = torch.stack(res)\n",
    "    example = res[0]\n",
    "    mean = res.mean(dim=0)\n",
    "    print(\"mean\\n\", mean)\n",
    "    print(\"example\\n\", example)\n",
    "\n",
    "# LINEAR_trp_vs_kernel()\n",
    "# # exact\n",
    "# #  tensor([[ 4627.0444, -5744.9629],\n",
    "# #         [ 9709.9150, -7802.3965],\n",
    "# #         [-1727.5579,  2563.8979]], device='cuda:0')\n",
    "# # 100%|██████████| 10000/10000 [00:15<00:00, 641.01it/s] # n_features = 500, trunc_level = 5\n",
    "# # mean\n",
    "# #  tensor([[ 4624.3896, -5737.0781],\n",
    "# #         [ 9733.1250, -7796.5479],\n",
    "# #         [-1733.9088,  2575.4778]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#### Gaussian TRP-RFSF features  vs  SigRBF kernel ####\n",
    "#######################################################\n",
    "\n",
    "def GAUSSIAN_trp_vs_kernel():\n",
    "    #parameters\n",
    "    N = 3\n",
    "    N2 = 2\n",
    "    T = 20\n",
    "    d = 2\n",
    "    trunc_level = 5\n",
    "    n_features = 5000\n",
    "    sigma = 1.0\n",
    "    dtype = torch.float32\n",
    "    #torch.manual_seed(3)\n",
    "    X = torch.randn(N, T, d, dtype=dtype).to(\"cuda\").detach() / np.sqrt(d)\n",
    "    Y = torch.randn(N2,T, d, dtype=dtype).to(\"cuda\").detach() / np.sqrt(d)\n",
    "\n",
    "    #exact sig kernel\n",
    "    sigker = TruncSigKernel(RBFKernel(sigma=sigma), normalize=False, trunc_level=trunc_level, geo_order=1, max_batch=50000)\n",
    "    K = sigker(X,Y)\n",
    "    print(\"exact\\n\", K)\n",
    "\n",
    "    #trp\n",
    "    MC_iter = 10000\n",
    "    res = []\n",
    "    for i in tqdm(range(MC_iter)):\n",
    "        trp = TRP_RFSF_Gaussian(trunc_level, n_features, sigma, only_last=True)\n",
    "        feat_X = trp(X)\n",
    "        feat_Y = trp(Y)\n",
    "        K_trp = 1 + feat_X @ feat_Y.T\n",
    "        res.append(K_trp)\n",
    "    res = torch.stack(res)\n",
    "    example = res[0]\n",
    "    mean = res.mean(dim=0)\n",
    "    print(\"mean\\n\", mean)\n",
    "    print(\"example\\n\", example)\n",
    "\n",
    "# GAUSSIAN_trp_vs_kernel()\n",
    "# # exact\n",
    "# #  tensor([[ -8.7953, -30.1428],\n",
    "# #         [ -4.1622,  -4.5515],\n",
    "# #         [-25.4650,  42.8830]], device='cuda:0')\n",
    "# # 100%|██████████| 10000/10000 [05:55<00:00, 28.10it/s] # n_features=5000, trunc_level=5\n",
    "# # mean\n",
    "# #  tensor([[ -9.9236, -30.5771],\n",
    "# #         [ -4.4291,  -4.5940],\n",
    "# #         [-25.3243,  43.3625]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#### Test RBF_RandomFourierFeatures vs exact RBF kernel ####\n",
    "############################################################\n",
    "def rff_vs_exact_RBFKernel():\n",
    "    N=3\n",
    "    N2= 2\n",
    "    d = 10\n",
    "    sigma=1\n",
    "    dtype = torch.float64\n",
    "    # torch.manual_seed(1)\n",
    "    X = torch.randn(N, d, dtype=dtype).to(\"cuda\") /np.sqrt(d)\n",
    "    Y = torch.randn(N2, d, dtype=dtype).to(\"cuda\") / np.sqrt(d)\n",
    "\n",
    "    # Exact RBF kernel\n",
    "    k = RBFKernel(sigma=sigma)\n",
    "    K = k(X, Y)\n",
    "\n",
    "    # Approximate RBF kernel using RBF_RandomFourierFeatures\n",
    "    N_MC = 10000\n",
    "    res = []\n",
    "    for i in range(N_MC):\n",
    "        RFF = RBF_RandomFourierFeatures(n_features=1000,\n",
    "                                        sigma=sigma,\n",
    "                                        method=\"cos(x)sin(x)\",\n",
    "                                        # method = \"cos(x + b)\",\n",
    "                                        )\n",
    "        feat_X = RFF(X)\n",
    "        feat_Y = RFF(Y)\n",
    "        K_rff = feat_X @ feat_Y.T\n",
    "        res.append(K_rff)\n",
    "    K_rff = torch.mean(torch.stack(res), dim=0)\n",
    "\n",
    "    print(\"K\\n\",K)\n",
    "    print(\"K_rff\\n\",K_rff)\n",
    "    print(\"diff\\n\", K-K_rff)\n",
    "    print(\"diffmean\\n\", torch.mean(abs(K-K_rff)))\n",
    "    # the RFF approach cant reproduce results smaller than 1e-5 for some reason\n",
    "    \n",
    "# rff_vs_exact_RBFKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "######### FROM KSIG LIBRARY #########\n",
    "#####################################\n",
    "import numpy as np\n",
    "import ksig\n",
    "\n",
    "def ksig_readme():\n",
    "    # Number of signature levels to use.\n",
    "    n_levels = 5 \n",
    "    normalize=False\n",
    "\n",
    "    # Use 100 components in RFF and projection.\n",
    "    n_components = 100\n",
    "\n",
    "    # Instantiate RFF feature map.\n",
    "    static_feat = ksig.static.features.RandomFourierFeatures(n_components=n_components)\n",
    "    # Instantiate tensor random projections.\n",
    "    proj = ksig.projections.TensorizedRandomProjection(n_components=n_components)\n",
    "\n",
    "    # The RFSF-TRP feature map and kernel. Additionally to working as a callable for\n",
    "    # computing a kernel, it implements a fit and a transform method.\n",
    "    rfsf_trp_kernel = ksig.kernels.SignatureFeatures(\n",
    "        n_levels=n_levels, static_features=static_feat, projection=proj, normalize=normalize)\n",
    "\n",
    "    # Generate 1000 sequences of length 200 with 100 features.\n",
    "    n_seq, l_seq, n_feat = 3, 20, 10\n",
    "    X = np.random.randn(n_seq, l_seq, n_feat) / np.sqrt(n_feat)\n",
    "\n",
    "    # Fit the kernel to the data.\n",
    "    rfsf_trp_kernel.fit(X)\n",
    "\n",
    "    # Compute the kernel matrix as before.\n",
    "    K_XX = rfsf_trp_kernel(X)  # K_XX has shape (1000, 1000).\n",
    "\n",
    "    # GEnerate another array of 800 sequences of length 250 and 100 features.\n",
    "    n_seq2, l_seq2 = 4, 20\n",
    "    Y = np.random.randn(n_seq2, l_seq2, n_feat) / np.sqrt(n_feat)\n",
    "\n",
    "    # Compute the kernel matrix between X and Y.\n",
    "    # The kernel does not have to be fitted a second time.\n",
    "    K_XY = rfsf_trp_kernel(X, Y)  # K_XY has shape (1000, 800)\n",
    "\n",
    "    # Alternatively, we may compute features separately for X and Y. Under the hood,\n",
    "    # this is what the call method does, i.e. compute features and take their inner product.\n",
    "    P_X = rfsf_trp_kernel.transform(X)  # P_X has shape (1000, 501)\n",
    "    P_Y = rfsf_trp_kernel.transform(Y)  # P_Y shape shape (800, 501)\n",
    "    print(\"P_X\", P_X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Use the RBF kernel for vector-valued data as static (base) kernel.\n",
    "    static_kernel = ksig.static.kernels.RBFKernel()\n",
    "    order = 1\n",
    "    sig_kernel = ksig.kernels.SignatureKernel(n_levels=n_levels, order=order, static_kernel=static_kernel,\n",
    "                                              normalize=normalize)\n",
    "\n",
    "    # Sequence kernels take as input an array of sequences of ndim == 3,\n",
    "    # and work as a callable for computing the kernel matrix.\n",
    "    K_XX_exact = sig_kernel(X)\n",
    "    K_XY_exact = sig_kernel(X, Y)  \n",
    "    print(\"\\n\\nTRP XX\\n\", K_XX)\n",
    "    print(\"Exact XX\\n\", K_XX_exact)\n",
    "    print(\"\\n\\nTRP XY\\n\", K_XY)\n",
    "    print(\"Exact XY\\n\", K_XY_exact)\n",
    "\n",
    "# ksig_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "## KSIG test TRP vs DP ##\n",
    "#########################\n",
    "import ksig\n",
    "import numpy as np\n",
    "\n",
    "def trp_vs_dp():\n",
    "    n_seq, l_seq, n_feat = 10, 150, 100\n",
    "    X = np.random.randn(n_seq, l_seq, n_feat)\n",
    "    n_levels = 10  # Number of signature levels to use.\n",
    "    n_components = 100 # Use dimension of RFF map\n",
    "\n",
    "    # Instantiate RFF feature map.\n",
    "    static_feat = ksig.static.features.RandomFourierFeatures(n_components=n_components)\n",
    "    trp_proj = ksig.projections.TensorizedRandomProjection(n_components=n_components)\n",
    "    dp_proj = ksig.projections.DiagonalProjection()\n",
    "\n",
    "    # The RFSF-TRP feature map and kernel. Additionally to working as a callable for\n",
    "    # computing a kernel, it implements a fit and a transform method.\n",
    "    trp_kernel = ksig.kernels.SignatureFeatures(\n",
    "        n_levels=n_levels, \n",
    "        static_features=static_feat, \n",
    "        projection=trp_proj\n",
    "        )\n",
    "    dp_kernel = ksig.kernels.SignatureFeatures(\n",
    "        n_levels=n_levels,\n",
    "        static_features=static_feat,\n",
    "        projection=dp_proj\n",
    "        )\n",
    "    trp_kernel.fit(X)\n",
    "    dp_kernel.fit(X)\n",
    "\n",
    "\n",
    "    # TIME IT\n",
    "    import timeit\n",
    "    def function_dp():\n",
    "        dp_kernel(X, X)\n",
    "    def function_trp():\n",
    "        trp_kernel(X, X)\n",
    "    execution_time_dp = timeit.timeit(function_dp, number=10)\n",
    "    print(\"Execution time of dp\\t:\", execution_time_dp)\n",
    "    execution_time_trp = timeit.timeit(function_trp, number=10)\n",
    "    print(\"Execution time of trp\\t:\", execution_time_trp)\n",
    "    \n",
    "#trp_vs_dp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
