{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Any, Optional, Tuple, Literal, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import sigkernel\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from kernels.static_kernels import StaticKernel, AbstractKernel, RBFKernel\n",
    "from kernels.integral_type import IntegralKernel\n",
    "from kernels.sig_pde import SigPDEKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([5, 7, 3])\n",
      "Y torch.Size([6, 7, 3])\n",
      "Z torch.Size([5, 7, 3])\n",
      "Testing <kernels.integral_type.IntegralKernel object at 0x7f689a3d0250>\n",
      "gram torch.Size([5, 6])\n",
      "diag torch.Size([5])\n",
      "batch_call torch.Size([5])\n",
      "call torch.Size([1]) tensor([0.7511], dtype=torch.float64)\n",
      "\n",
      "\n",
      "Testing <kernels.sig_pde.SigPDEKernel object at 0x7f698d721290>\n",
      "gram torch.Size([5, 6])\n",
      "diag torch.Size([5])\n",
      "batch_call torch.Size([5])\n",
      "call torch.Size([1]) tensor([1.4211], dtype=torch.float64)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_kernel(kernel: AbstractKernel):\n",
    "    print(\"Testing\", kernel)\n",
    "\n",
    "    gram = kernel.gram(X, Y)\n",
    "    diag = kernel.gram(X, Z, diag=True)\n",
    "    batch_call = kernel(X, Z)\n",
    "    call = kernel(X[0], Z[0])\n",
    "    print(\"gram\", gram.shape)\n",
    "    print(\"diag\", diag.shape)\n",
    "    print(\"batch_call\", batch_call.shape)\n",
    "    print(\"call\", call.shape, call)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "N1 = 5\n",
    "N2 = 6\n",
    "T = 7\n",
    "d = 3\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand((N1, T, d), dtype=torch.float64)\n",
    "Y = torch.rand((N2, T, d), dtype=torch.float64)\n",
    "Z = torch.rand((N1, T, d), dtype=torch.float64)\n",
    "print(\"X\", X.shape)\n",
    "print(\"Y\", Y.shape)\n",
    "print(\"Z\", Z.shape)\n",
    "\n",
    "rbf = RBFKernel()\n",
    "integral = IntegralKernel(rbf)\n",
    "sigpde = SigPDEKernel(rbf)\n",
    "test_kernel(integral)\n",
    "test_kernel(sigpde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def sig_kernel(s1:Tensor, \n",
    "               s2:Tensor, \n",
    "               order:int,\n",
    "               static_kernel_gram:Callable = linear_kernel_gram,\n",
    "               only_last:bool = True):\n",
    "    \"\"\"Computes the truncated signature kernel of two time series of \n",
    "    shape (T_i, d) with respect to a static kernel on R^d.\n",
    "\n",
    "    Args:\n",
    "        s1 (np.ndarray): Array of shape (T_1, d).\n",
    "        s2 (np.ndarray): Array of shape (T_2, d).\n",
    "        order (int): Truncation order of the signature kernel.\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays,\n",
    "                            see e.g. 'linear_kernel_gram' or 'rbf_kernel_gram'.\n",
    "        only_last (bool): If False, returns results of all truncation levels up to 'order'.\n",
    "    \"\"\"\n",
    "    K = static_kernel_gram(s1, s2)\n",
    "    nabla = K[1:, 1:] + K[:-1, :-1] - K[1:, :-1] - K[:-1, 1:]\n",
    "    sig_kers = jitted_trunc_sig_kernel(nabla, order)\n",
    "    if only_last:\n",
    "        return sig_kers[-1]\n",
    "    else:\n",
    "        return sig_kers\n",
    "\n",
    "\n",
    "\n",
    "@njit((nb.float64[:, ::1], nb.int64), fastmath=True, cache=True)\n",
    "def reverse_cumsum(arr:Tensor, axis:int): #ndim=2\n",
    "    \"\"\"JITed reverse cumulative sum along the specified axis.\n",
    "    (np.cumsum with axis is not natively supported by Numba)\n",
    "    \n",
    "    Args:\n",
    "        arr (np.ndarray): Array of shape (T_1, T_2).\n",
    "        axis (int): Axis along which to cumsum.\n",
    "    \"\"\"\n",
    "    A = arr.copy()\n",
    "    if axis==0:\n",
    "        for i in np.arange(A.shape[0]-2, -1, -1):\n",
    "            A[i, :] += A[i+1, :]\n",
    "    else: #axis==1\n",
    "        for i in np.arange(A.shape[1]-2, -1, -1):\n",
    "            A[:,i] += A[:,i+1]\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "@njit((nb.float64[:, ::1], nb.int64), fastmath=True, cache=True)\n",
    "def jitted_trunc_sig_kernel(nabla, order):\n",
    "    \"\"\"Given difference matrix nabla_ij = K[i+1, j+1] + K[i, j] - K[i+1, j] - K[i, j+1],\n",
    "    computes the truncated signature kernel of all orders up to 'order'.\"\"\"\n",
    "    B = np.ones((order+1, order+1, order+1, *nabla.shape))\n",
    "    for d in np.arange(order):\n",
    "        for n in np.arange(order-d):\n",
    "            for m in np.arange(order-d):\n",
    "                B[d+1,n,m] = 1 + nabla/(n+1)/(m+1)*B[d, n+1, m+1]\n",
    "                r1 = reverse_cumsum(nabla * B[d, n+1, 1] / (n+1), axis=0)\n",
    "                B[d+1,n,m, :-1, :] += r1[1:, :]\n",
    "                r2 = reverse_cumsum(nabla * B[d, 1, m+1] / (m+1), axis=1)\n",
    "                B[d+1,n,m, :, :-1] += r2[:, 1:]\n",
    "                rr = reverse_cumsum(nabla * B[d, 1, 1], axis=0)\n",
    "                rr = reverse_cumsum(rr, axis=1)\n",
    "                B[d+1,n,m, :-1, :-1] += rr[1:, 1:]\n",
    "\n",
    "    #copy, otherwise all memory accumulates in for loop\n",
    "    return B[1:,0,0,0,0].copy() \n",
    "\n",
    "\n",
    "\n",
    "def sig_kernel_gram(\n",
    "        X:List[np.ndarray],\n",
    "        Y:List[np.ndarray],\n",
    "        order:int,\n",
    "        static_kernel_gram:Callable,\n",
    "        only_last:bool = True,\n",
    "        sym:bool = False,\n",
    "        n_jobs:int = 1,\n",
    "        verbose:bool = False,\n",
    "    ):\n",
    "    \"\"\"Computes the Gram matrix k_sig(X_i, Y_j) of the signature kernel,\n",
    "    given the static kernel k(x, y) and the truncation order.\n",
    "\n",
    "    Args:\n",
    "        X (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        Y (List[np.ndarray]): List of time series of shape (T_j, d).\n",
    "        order (int): Truncation level of the signature kernel.\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays,\n",
    "                            see e.g. 'linear_kernel_gram' or 'rbf_kernel_gram'.\n",
    "        only_last (bool): If False, returns results of all truncation levels up to 'order'.\n",
    "        sym (bool): If True, computes the symmetric Gram matrix.\n",
    "        n_jobs (int): Number of parallel jobs to run.\n",
    "        verbose (bool): Whether to enable the tqdm progress bar.\n",
    "    \"\"\"\n",
    "    pairwise_ker = lambda s1, s2 : sig_kernel(s1, s2, order, static_kernel_gram, only_last)\n",
    "    return pairwise_kernel_gram(X, Y, pairwise_ker, sym, n_jobs, verbose)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
