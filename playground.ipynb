{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Any, Optional, Tuple, Literal, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import sigkernel\n",
    "import os\n",
    "import sys\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "import ksig\n",
    "\n",
    "from kernels.abstract_base import TimeSeriesKernel, StaticKernel\n",
    "from kernels.static_kernels import LinearKernel, RBFKernel, PolyKernel\n",
    "from kernels.integral import StaticIntegralKernel\n",
    "from kernels.sig_pde import SigPDEKernel\n",
    "from kernels.sig_trunc import TruncSigKernel\n",
    "from kernels.gak import GlobalAlignmentKernel, sigma_gak\n",
    "from kernels.flattened_static import FlattenedStaticKernel\n",
    "from kernels.reservoir import ReservoirKernel\n",
    "\n",
    "from features.signature import sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T=30, d=2\n",
      "path development n_features=200, n_MC = 10\n",
      "tensor([[  2723383.8437,   4388336.5323],\n",
      "        [ -3114313.6835,   -505699.7463],\n",
      "        [-36695762.9923,  -2316757.4759]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "\n",
      "pde sigker dyadic_order=3\n",
      "tensor([[   5937.1830, 3242989.9209],\n",
      "        [  -6623.9208,  158561.4732],\n",
      "        [ 595235.0906, -271875.4427]], device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "non-geometric trunc sigker trunc_level=15\n",
      "tensor([[-22475.6242,  -3424.4065],\n",
      "        [  -866.7473,   -988.0156],\n",
      "        [   343.7322,   3667.5261]], device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "piecewise linear trunc sigker trunc_level=15\n",
      "tensor([[   5331.2835, 3033761.0354],\n",
      "        [  -6323.8684,  139105.0875],\n",
      "        [ 390713.5333, -173587.0705]], device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "exact signature inner product trunc_level=15\n",
      "tensor([[   5330.2835, 3033760.0354],\n",
      "        [  -6324.8684,  139104.0875],\n",
      "        [ 390712.5333, -173588.0705]], device='cuda:0', dtype=torch.float64)\n",
      "Execution time of function: 0.6131405620035366\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#########    randomized sigs   ###########\n",
    "##########################################\n",
    "\n",
    "\n",
    "from typing import List, Dict, Set, Any, Optional, Tuple, Literal, Callable\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.functional import tanh\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from kernels.abstract_base import TimeSeriesKernel, StaticKernel\n",
    "from kernels.static_kernels import RBFKernel\n",
    "\n",
    "    \n",
    "\n",
    "def randomized_sig(\n",
    "        X:Tensor,\n",
    "        A:Tensor,\n",
    "        b:Tensor,\n",
    "        Y_0:Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Randomized signature of a (batched) time series X, with identity\n",
    "    activation function.\n",
    "\n",
    "    Args:\n",
    "        X (Tensor): Input tensor of shape (N, T, d).\n",
    "        A (Tensor): Tensor of shape (M, M, d). Random matrix.\n",
    "        b (Tensor): Tensor of shape (M, d). Random bias.\n",
    "        Y_0 (Tensor): Initial value of the randomized signature.\n",
    "            Tensor of shape (M).\n",
    "    \"\"\"\n",
    "    N, T, d = X.shape\n",
    "    diff = X.diff(dim=-2) # shape (N, T-1, d)\n",
    "    Y_0 = torch.tile(Y_0, (N, 1)) # shape (N, M)\n",
    "\n",
    "    #iterate y[t+1] = y[t] + ...\n",
    "    Z = torch.tensordot(Y_0, A, dims=1) + b #shape (N, M, d)\n",
    "    Y = Y_0 + (Z * diff[:, 0:1, :]).sum(dim=-1) # shape (N, M)\n",
    "    for t in range(1, T-1):\n",
    "        Z = torch.tensordot(Y, A, dims=1) + b\n",
    "        Y = Y + (Z * diff[:, t:t+1, :]).sum(dim=-1)\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "def randomized_sig_ReLU(\n",
    "        X:Tensor,\n",
    "        A:Tensor,\n",
    "        b:Tensor,\n",
    "        Y_0:Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Randomized signature of a (batched) time series X, with ReLU\n",
    "    activation function.\n",
    "\n",
    "    Args:\n",
    "        X (Tensor): Input tensor of shape (N, T, d).\n",
    "        A (Tensor): Tensor of shape (M, M, d). Random matrix.\n",
    "        b (Tensor): Tensor of shape (M, d). Random bias.\n",
    "        Y_0 (Tensor): Initial value of the randomized signature.\n",
    "            Tensor of shape (M).\n",
    "    \"\"\"\n",
    "    N, T, d = X.shape\n",
    "    diff = X.diff(dim=1) # shape (N, T-1, d)\n",
    "    Y_0 = torch.tile(Y_0, (N, 1)) # shape (N, M)\n",
    "\n",
    "    #iterate y[t+1] = y[t] + ...\n",
    "    Z = torch.tensordot(relu(Y_0), A, dims=1) + b[None] # shape (N, M, d)\n",
    "    Y = Y_0 + (Z * diff[:, 0, :].unsqueeze(-2)).sum(dim=-1) # shape (N, M)\n",
    "    for t in range(1, T-1):\n",
    "        Z = torch.tensordot(relu(Y), A, dims=1) + b[None]\n",
    "        Y = Y + (Z * diff[:, t, :].unsqueeze(-2)).sum(dim=-1)\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "def randomized_sig_tanh(\n",
    "        X:Tensor,\n",
    "        A:Tensor,\n",
    "        b:Tensor,\n",
    "        Y_0:Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Randomized signature of a (batched) time series X, with tanh\n",
    "    activation function.\n",
    "\n",
    "    Args:\n",
    "        X (Tensor): Input tensor of shape (N, T, d).\n",
    "        A (Tensor): Tensor of shape (M, M, d). Random matrix.\n",
    "        b (Tensor): Tensor of shape (M, d). Random bias.\n",
    "        Y_0 (Tensor): Initial value of the randomized signature.\n",
    "            Tensor of shape (M).\n",
    "    \"\"\"\n",
    "    N, T, d = X.shape\n",
    "    diff = X.diff(dim=1) # shape (N, T-1, d)\n",
    "    Y_0 = torch.tile(Y_0, (N, 1)) # shape (N, M)\n",
    "\n",
    "    #iterate y[t+1] = y[t] + ...\n",
    "    Z = torch.tensordot(tanh(Y_0), A, dims=1) + b[None] # shape (N, M, d)\n",
    "    Y = Y_0 + (Z * diff[:, 0, :].unsqueeze(-2)).sum(dim=-1) # shape (N, M)\n",
    "    for t in range(1, T-1):\n",
    "        Z = torch.tensordot(tanh(Y), A, dims=1) + b[None]\n",
    "        Y = Y + (Z * diff[:, t, :].unsqueeze(-2)).sum(dim=-1)\n",
    "    return Y\n",
    "\n",
    "\n",
    "class RandomizedSigKernel(TimeSeriesKernel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features = 100,\n",
    "            activation:Literal[\"identity\", \"relu\", \"tanh\"] = \"relu\",\n",
    "            seed:int = 0,\n",
    "            max_batch:int = 1000,\n",
    "            normalize:bool = False,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        The randomized signature kernel of two time series of \n",
    "        shape (T_i, d).\n",
    "\n",
    "        Args:\n",
    "            n_features (int): Number of features.\n",
    "            activation (str): Activation function.\n",
    "            seed (int): Random seed.\n",
    "            max_batch (int, optional): Max batch size for computations.\n",
    "            normalize (bool, optional): If True, normalizes the kernel.\n",
    "        \"\"\"\n",
    "        super().__init__(max_batch, normalize)\n",
    "        self.n_features = n_features\n",
    "        self.activation = activation\n",
    "        self.seed = seed\n",
    "        self.has_initialized = False\n",
    "\n",
    "\n",
    "    def _init_given_input(\n",
    "            self, \n",
    "            X: Tensor\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes the random matrices and biases used in the \n",
    "        randomized signature kernel.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Example input tensor of shape (N, T, d) of \n",
    "                timeseries.\n",
    "        \"\"\"\n",
    "        # Get shape, dtype and device info.\n",
    "        N, T, d = X.shape\n",
    "        device = X.device\n",
    "        dtype = X.dtype\n",
    "        \n",
    "        # Create a generator and set the seed\n",
    "        gen = torch.Generator(device=device).manual_seed(self.seed)\n",
    "        \n",
    "        # Initialize the random matrices and biases\n",
    "        self.A = torch.randn(self.n_features, \n",
    "                             self.n_features, \n",
    "                             d, \n",
    "                             device=device,\n",
    "                             dtype=dtype,\n",
    "                             generator=gen\n",
    "                             ) / np.sqrt(self.n_features)\n",
    "        # self.b = torch.randn(self.n_features,\n",
    "        #                      d,\n",
    "        #                      device=device,\n",
    "        #                      dtype=dtype,\n",
    "        #                      generator=gen\n",
    "        #                      )\n",
    "        self.b = torch.zeros(self.n_features,\n",
    "                             d,\n",
    "                             device=device,\n",
    "                             dtype=dtype,)\n",
    "\n",
    "        self.Y_0 = torch.randn(self.n_features,\n",
    "                               device=device,\n",
    "                               dtype=dtype,\n",
    "                               generator=gen)\n",
    "\n",
    "\n",
    "    def _gram(\n",
    "            self, \n",
    "            X: Tensor, \n",
    "            Y: Tensor,\n",
    "            diag: bool,\n",
    "        ):\n",
    "        if not self.has_initialized:\n",
    "            self._init_given_input(X)\n",
    "            self.has_initialized = True\n",
    "\n",
    "        fun = randomized_sig if self.activation == \"identity\" else \\\n",
    "              randomized_sig_ReLU if self.activation == \"relu\" else \\\n",
    "              randomized_sig_tanh\n",
    "        \n",
    "        feat_X = fun(X, self.A, self.b, self.Y_0)\n",
    "        feat_Y = fun(Y, self.A, self.b, self.Y_0)\n",
    "\n",
    "        if diag:\n",
    "            return (feat_X * feat_Y).mean(dim=-1)\n",
    "        else:\n",
    "            return feat_X @ feat_Y.t() / self.n_features\n",
    "        \n",
    "\n",
    "#############\n",
    "## test it ##\n",
    "#############\n",
    "\n",
    "\n",
    "def test_randsig():\n",
    "    N= 3\n",
    "    N2= 2\n",
    "    T, d = 30, 2\n",
    "    torch.manual_seed(0)\n",
    "    X = torch.randn(N,  T, d, dtype=torch.float64).to(\"cuda\").cumsum(dim=1) / np.sqrt(d)\n",
    "    Y = torch.randn(N2, T, d, dtype=torch.float64).to(\"cuda\").cumsum(dim=1) / np.sqrt(d)\n",
    "\n",
    "    n_features = 200\n",
    "    n_MC = 10\n",
    "    out = []\n",
    "    for i in range(n_MC):\n",
    "        ker = RandomizedSigKernel(n_features=n_features, \n",
    "                                activation=\"identity\", \n",
    "                                normalize=False)\n",
    "        out.append(ker(X, Y))\n",
    "    out = torch.stack(out).mean(dim=0)\n",
    "    print(f\"\\nT={T}, d={d}\")\n",
    "    print(f\"path development n_features={n_features}, n_MC = {n_MC}\")\n",
    "    print(out)\n",
    "\n",
    "    dyadic_order = 3\n",
    "    sigkernel = SigPDEKernel(LinearKernel(),\n",
    "                            dyadic_order=dyadic_order,\n",
    "                                normalize=False)\n",
    "    out = sigkernel(X, Y)\n",
    "    print(f\"\\npde sigker dyadic_order={dyadic_order}\")\n",
    "    print(out)\n",
    "\n",
    "    trunc_level=15\n",
    "    sigtrunc = TruncSigKernel(LinearKernel(),\n",
    "                            trunc_level=trunc_level,\n",
    "                            geo_order = 1,)\n",
    "    out = sigtrunc(X, Y)\n",
    "    print(f\"\\nnon-geometric trunc sigker trunc_level={trunc_level}\")\n",
    "    print(out)\n",
    "\n",
    "    lineartruncsig = TruncSigKernel(LinearKernel(),\n",
    "                            trunc_level=trunc_level,\n",
    "                            geo_order = trunc_level,)\n",
    "    out = lineartruncsig(X, Y)\n",
    "    print(f\"\\npiecewise linear trunc sigker trunc_level={trunc_level}\")\n",
    "    print(out)\n",
    "\n",
    "    sig_X = sig(X, trunc_level)\n",
    "    sig_Y = sig(Y, trunc_level)\n",
    "    out = sig_X @ sig_Y.t()\n",
    "    print(f\"\\nexact signature inner product trunc_level={trunc_level}\")\n",
    "    print(out)\n",
    "\n",
    "    def function():\n",
    "        ker(X, Y)\n",
    "\n",
    "    import timeit\n",
    "    # Measure the execution time of function\n",
    "    execution_time = timeit.timeit(function, number=100)\n",
    "    print(\"Execution time of function:\", execution_time)\n",
    "\n",
    "test_randsig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive\n",
      " tensor([[ -4.8191,   1.1076,  -3.6456,  -5.6884,   6.6305,   5.9853,   5.5638,\n",
      "           3.1855,   4.8021,  -2.7075,  -1.8026,   4.1848,  -1.7182,   0.5929,\n",
      "          13.5506,   5.5576,   3.0846,   1.1335,  -5.0735,  -5.3065,  -0.7329,\n",
      "           0.7478,  -1.2011,  -3.3103,   3.6241,  -2.9897,  -9.7440,  -2.9861,\n",
      "          -9.8475,  -3.4611,  -1.9100,  -7.3128,  -2.5018,  -5.7883,  -8.3317,\n",
      "           1.8492,  -3.7105,  -9.3530,  -1.3751,  -1.7755],\n",
      "        [ -7.2429,  11.6536,   1.8351,  -5.6309,  12.3161,  -6.3678,  -0.6727,\n",
      "          -5.2734,  -0.4374,  -2.5425,   2.1544,  -6.5285,   6.8743,  -9.8720,\n",
      "         -12.8083,  -5.6890,  -8.5065,  -2.4751,  10.6474,   8.8322,   8.1747,\n",
      "          -8.2879,   9.5290,   5.3229,  -7.5562,  -1.0465,   2.3181,   4.8677,\n",
      "          -0.3393,  -3.5337,  -0.0770,   9.1050,  17.1236, -11.5461,  -4.3280,\n",
      "          -4.0708,   0.2352,  -4.9608,   8.0505, -12.4527],\n",
      "        [  4.3682,  -3.6289,  -1.8262, -10.1565,   0.3523,   6.9981,  -3.7521,\n",
      "         -22.4437,  -5.7022,   4.8658,  14.3040,   2.8968,  14.2112,  16.8928,\n",
      "          -6.8566,  -4.1116,   1.0986,  11.1577,  -6.3931,   1.2091,  13.3912,\n",
      "          13.4210,   3.2531,   4.7306,   8.8607,  -4.8390,   5.0446,   2.0755,\n",
      "           0.6858,  13.0612,   3.4703,   3.1023,  -3.2153,   0.1241,   4.4695,\n",
      "           0.8732,   8.6809,   0.6659,   2.0200,   1.8300]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "out\n",
      " tensor([[ -4.8191,   1.1076,  -3.6456,  -5.6884,   6.6305,   5.9853,   5.5638,\n",
      "           3.1855,   4.8021,  -2.7075,  -1.8026,   4.1848,  -1.7182,   0.5929,\n",
      "          13.5506,   5.5576,   3.0846,   1.1335,  -5.0735,  -5.3065,  -0.7329,\n",
      "           0.7478,  -1.2011,  -3.3103,   3.6241,  -2.9897,  -9.7440,  -2.9861,\n",
      "          -9.8475,  -3.4611,  -1.9100,  -7.3128,  -2.5018,  -5.7883,  -8.3317,\n",
      "           1.8492,  -3.7105,  -9.3530,  -1.3751,  -1.7755],\n",
      "        [ -7.2429,  11.6536,   1.8351,  -5.6309,  12.3161,  -6.3678,  -0.6727,\n",
      "          -5.2734,  -0.4374,  -2.5425,   2.1544,  -6.5285,   6.8743,  -9.8720,\n",
      "         -12.8083,  -5.6890,  -8.5065,  -2.4751,  10.6474,   8.8322,   8.1747,\n",
      "          -8.2879,   9.5290,   5.3229,  -7.5562,  -1.0465,   2.3181,   4.8677,\n",
      "          -0.3393,  -3.5337,  -0.0770,   9.1050,  17.1236, -11.5461,  -4.3280,\n",
      "          -4.0708,   0.2352,  -4.9608,   8.0505, -12.4527],\n",
      "        [  4.3682,  -3.6289,  -1.8262, -10.1565,   0.3523,   6.9981,  -3.7521,\n",
      "         -22.4437,  -5.7022,   4.8658,  14.3040,   2.8968,  14.2112,  16.8928,\n",
      "          -6.8566,  -4.1116,   1.0986,  11.1577,  -6.3931,   1.2091,  13.3912,\n",
      "          13.4210,   3.2531,   4.7306,   8.8607,  -4.8390,   5.0446,   2.0755,\n",
      "           0.6858,  13.0612,   3.4703,   3.1023,  -3.2153,   0.1241,   4.4695,\n",
      "           0.8732,   8.6809,   0.6659,   2.0200,   1.8300]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "diff tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# test naive randsig\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import tanh\n",
    "from typing import List, Dict, Set, Any, Optional, Tuple, Literal, Callable\n",
    "\n",
    "@torch.jit.script\n",
    "def randomized_sig_tanh(\n",
    "        X:Tensor,\n",
    "        A:Tensor,\n",
    "        b:Tensor,\n",
    "        Y_0:Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Randomized signature of a (batched) time series X, with tanh\n",
    "    activation function.\n",
    "\n",
    "    Args:\n",
    "        X (Tensor): Input tensor of shape (N, T, d).\n",
    "        A (Tensor): Tensor of shape (M, M, d). Random matrix.\n",
    "        b (Tensor): Tensor of shape (M, d). Random bias.\n",
    "        Y_0 (Tensor): Initial value of the randomized signature.\n",
    "            Tensor of shape (M).\n",
    "    \"\"\"\n",
    "    N, T, d = X.shape\n",
    "    diff = X.diff(dim=1) # shape (N, T-1, d)\n",
    "    Y_0 = torch.tile(Y_0, (N, 1)) # shape (N, M)\n",
    "\n",
    "    #iterate y[t+1] = y[t] + ...\n",
    "    Z = torch.tensordot(tanh(Y_0), A, dims=1) + b[None] # shape (N, M, d)\n",
    "    Y = Y_0 + (Z * diff[:, 0:1, :]).sum(dim=-1) # shape (N, M)\n",
    "    for t in range(1, T-1):\n",
    "        Z = torch.tensordot(tanh(Y), A, dims=1) + b[None]\n",
    "        Y = Y + (Z * diff[:, t:t+1, :]).sum(dim=-1)\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "def naive_rand_sig(\n",
    "        X:Tensor,\n",
    "        A:Tensor,\n",
    "        b:Tensor,\n",
    "        Y_0:Tensor,\n",
    "        activation:Callable,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    The naive non-optimized version of rand sigs\n",
    "\n",
    "    Args:\n",
    "        X (Tensor): Input tensor of shape (N, T, d).\n",
    "        A (Tensor): Tensor of shape (M, M, d). Random matrix.\n",
    "        b (Tensor): Tensor of shape (M, d). Random bias.\n",
    "        Y_0 (Tensor): Initial value of the randomized signature.\n",
    "            Tensor of shape (M).\n",
    "        activation (Callable): Activation function.\n",
    "    \"\"\"\n",
    "    N, T, d = X.shape\n",
    "    diff = X.diff(dim=-2) # shape (N, T-1, d)\n",
    "    Y = Y_0[None].repeat(N, 1) # shape (N, M)\n",
    "    Z = Y.clone()\n",
    "\n",
    "    for t in range(T-1):\n",
    "        v = [] #for each dim of the control path\n",
    "        for k in range(d):\n",
    "            summand =  (activation(Z) @ A[:,:, k] + b[None, :, k])\n",
    "            summand = summand * diff[:, t:t+1, k]\n",
    "            v.append(summand)\n",
    "        Z = Z + torch.stack(v, dim=-1).sum(dim=-1)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def test_naive_randsig():\n",
    "    N,T,d = 3, 30, 2\n",
    "    torch.manual_seed(0)\n",
    "    X = torch.randn(N,  T, d, dtype=torch.float64).to(\"cuda\").cumsum(dim=1) / np.sqrt(d)\n",
    "    n_features = 40\n",
    "    A = torch.randn(n_features, n_features, d, dtype=torch.float64).to(\"cuda\") / np.sqrt(n_features)\n",
    "    b = torch.zeros(n_features, d, dtype=torch.float64).to(\"cuda\")\n",
    "    Y_0 = torch.randn(n_features, dtype=torch.float64).to(\"cuda\")\n",
    "\n",
    "    out_naive = naive_rand_sig(X, A, b, Y_0, torch.tanh)\n",
    "    print(\"naive\\n\", out_naive)\n",
    "\n",
    "    out = randomized_sig_tanh(X, A, b, Y_0)\n",
    "    print(\"out\\n\", out)\n",
    "\n",
    "    print(\"diff\", out_naive - out)\n",
    "\n",
    "\n",
    "test_naive_randsig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ksig\n",
    "import timeit\n",
    "\n",
    "#### Test GAK ####\n",
    "N= 8\n",
    "N2= 20\n",
    "T, d = 20, 2\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(N,  T, d, dtype=torch.float64).to(\"cuda\") / d\n",
    "Y = torch.randn(N2, T, d, dtype=torch.float64).to(\"cuda\") / d\n",
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "\n",
    "sigma = tslearn.metrics.sigma_gak(X_np)\n",
    "gak = tslearn.metrics.cdist_gak\n",
    "ksigker = ksig.kernels.GlobalAlignmentKernel(static_kernel=ksig.static.kernels.RBFKernel(bandwidth=sigma))\n",
    "mine = GlobalAlignmentKernel(RBFKernel(sigma=sigma), normalize=True, max_batch=50000)\n",
    "# ksigker = ksig.kernels.SignatureKernel(static_kernel=ksig.static.kernels.RBFKernel(bandwidth=sigma), n_levels=5, order=1)\n",
    "# mine = TruncSigKernel(RBFKernel(sigma=sigma), normalize=False, trunc_level=5, geo_order=1, max_batch=50000)\n",
    "# ksigker = ksig.kernels.SignatureKernel(static_kernel=ksig.static.kernels.LinearKernel(), n_levels=5, order=1)\n",
    "# mine = TruncSigKernel(LinearKernel(), normalize=True, trunc_level=5, geo_order=1, max_batch=50000)\n",
    "\n",
    "# print(tslearn.metrics.sigma_gak(X_np))\n",
    "# print(sigma_gak(X))\n",
    "# out = gak(X, X, sigma=sigma)\n",
    "# print(out)\n",
    "\n",
    "out2 = ksigker(X_np, X_np)\n",
    "print(out2)\n",
    "out3 = mine(X, X)\n",
    "#print(out)\n",
    "print(out3)\n",
    "print(np.mean(np.abs(out2 - out3.cpu().numpy())))\n",
    "\n",
    "def function1():\n",
    "    gak(X, X_np, sigma=sigma)\n",
    "\n",
    "def function2():\n",
    "    ksigker(X_np, X_np)\n",
    "\n",
    "def function3():\n",
    "    with torch.no_grad():\n",
    "        mine(X, Y)\n",
    "\n",
    "# # Measure the execution time of function 1\n",
    "# execution_time1 = timeit.timeit(function1, number=1)\n",
    "# print(\"Execution time of function 1:\", execution_time1)\n",
    "\n",
    "# Measure the execution time of function 2\n",
    "execution_time2 = timeit.timeit(function2, number=1)\n",
    "print(\"Execution time of function 2:\", execution_time2)\n",
    "# Measure the execution time of function 3\n",
    "execution_time3 = timeit.timeit(function3, number=1)\n",
    "print(\"Execution time of function 3:\", execution_time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over antidiagonals with s,t>0\n",
    "T1 = 5\n",
    "T2 = 4\n",
    "for diag in range(2, T1+T2-1):\n",
    "    for s in range(max(1, diag - T2 + 1), min(diag, T1)):\n",
    "        t = diag - s\n",
    "        print(s,t)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for loop indices\n",
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "from kernels.static_kernels import LinearKernel\n",
    "\n",
    "lin_ker = LinearKernel()\n",
    "\n",
    "def placeholder_ker(X:Tensor, Y:Tensor, diag:bool=False):\n",
    "    return lin_ker.time_gram(X, Y, diag)[...,0,0]\n",
    "\n",
    "\n",
    "def test_indices(X:Tensor, \n",
    "                 Y:Tensor,\n",
    "                 diag:bool,\n",
    "                max_batch:int, \n",
    "    ):\n",
    "    device = X.device\n",
    "    N1, T, d = X.shape\n",
    "    N2, _, _ = Y.shape\n",
    "\n",
    "    # split into batches. FASTEST METHOD NO BATCH\n",
    "    t1 = time.perf_counter()\n",
    "    result = placeholder_ker(X, Y)\n",
    "    t2 = time.perf_counter()\n",
    "    print(\"time NOBATCH\\t\", t1-t2)\n",
    "\n",
    "    # split into batches BY INDICES\n",
    "    t1 = time.perf_counter()\n",
    "    if diag:\n",
    "        indices = torch.arange(N1, device=device).tile(2,1) # shape (2, N)\n",
    "    else:\n",
    "        indices = torch.cartesian_prod(torch.arange(N1, device=device), \n",
    "                                    torch.arange(N2, device=device)).T #shape (2, N1*N2)\n",
    "    split = torch.split(indices, max_batch, dim=1)\n",
    "    result = [placeholder_ker(X[ix], Y[iy], diag=True) for ix,iy in split]\n",
    "    t2 = time.perf_counter()\n",
    "    print(\"time INDEX\\t\", t1-t2)\n",
    "\n",
    "    # split into batches VIA SPLIT\n",
    "    t1 = time.perf_counter()\n",
    "    split_X = torch.split(X, max_batch, dim=0)\n",
    "    Y_max_batch = max(1, max_batch//N1)\n",
    "    split_Y = torch.split(Y, Y_max_batch, dim=0)\n",
    "    result = [placeholder_ker(ix, iy) for ix,iy in itertools.product(split_X, split_Y)]\n",
    "    if max_batch >= N1:\n",
    "        result = torch.cat(result, dim=1)\n",
    "    else:\n",
    "        result = torch.cat(result, dim=0).reshape(N1, N2)\n",
    "    t2 = time.perf_counter()\n",
    "    print(\"time SPLIT\\t\", t1-t2)\n",
    "\n",
    "X = torch.randn(200, 7, 10)\n",
    "Y = torch.randn(300, 7, 10)\n",
    "test_indices(X, Y, False, 10000)\n",
    "\n",
    "#split = torch.split(X, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test dimensions of TimeSeriesKernels ####\n",
    "\n",
    "N=3\n",
    "N2=4\n",
    "T, d = 7, 5\n",
    "X = torch.randn(N, T, d) / d**0.5\n",
    "Y = torch.randn(N2, T, d) / d**0.5\n",
    "inputs = [\n",
    "    (X, X),\n",
    "    (X, Y),\n",
    "    (X[0], X[0]),\n",
    "    (X[0], Y[0]),\n",
    "    (X[0], Y),\n",
    "    (X, Y[0]),\n",
    "]\n",
    "diag_inputs = [\n",
    "    (X, X),\n",
    "    (Y, Y),\n",
    "    (X[:min(N,N2)], Y[:min(N,N2)]),\n",
    "    (X[0], X[0]),\n",
    "    (X[0], Y[0]),\n",
    "]\n",
    "def test_kernel(ker: TimeSeriesKernel, inputs, diag=False):\n",
    "    print(ker)\n",
    "    for X, Y in inputs:\n",
    "        out = ker(X, Y, diag, normalize=False)\n",
    "        out_normalize = ker(X, Y, diag, normalize=True)\n",
    "        # print(out, \"out\")\n",
    "        # print(out_normalize, \"out, normalize\")\n",
    "        print(out.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "sigker = TruncSigKernel(static_kernel=RBFKernel(), \n",
    "                        trunc_level=6, \n",
    "                        geo_order=1,\n",
    "                        only_last=False,)\n",
    "test_kernel(sigker, inputs)\n",
    "test_kernel(sigker, diag_inputs, True)\n",
    "\n",
    "sigpde = SigPDEKernel(static_kernel=RBFKernel(),\n",
    "                     dyadic_order=3,)\n",
    "test_kernel(sigpde, inputs)\n",
    "test_kernel(sigpde, diag_inputs, True)\n",
    "\n",
    "intker = StaticIntegralKernel(static_kernel=RBFKernel())\n",
    "test_kernel(intker, inputs)\n",
    "test_kernel(intker, diag_inputs, True)\n",
    "\n",
    "pde = SigPDEKernel(static_kernel=RBFKernel(), dyadic_order=2)\n",
    "test_kernel(pde, inputs)\n",
    "test_kernel(pde, diag_inputs, True)\n",
    "\n",
    "gak = GlobalAlignmentKernel(static_kernel=RBFKernel(sigma=sigma_gak(X)))\n",
    "test_kernel(gak, inputs)\n",
    "test_kernel(gak, diag_inputs, True)\n",
    "\n",
    "flat = FlattenedStaticKernel(static_kernel=LinearKernel())\n",
    "test_kernel(flat, inputs)\n",
    "test_kernel(flat, diag_inputs, True)\n",
    "\n",
    "res = ReservoirKernel()\n",
    "test_kernel(res, inputs)\n",
    "test_kernel(res, diag_inputs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ksig\n",
    "\n",
    "# Number of signature levels to use.\n",
    "n_levels = 5 \n",
    "\n",
    "# Use the RBF kernel for vector-valued data as static (base) kernel.\n",
    "static_kernel = ksig.static.kernels.RBFKernel() \n",
    "\n",
    "# Instantiate the signature kernel, which takes as input the static kernel.\n",
    "n_levels = 5\n",
    "order = 1\n",
    "sig_kernel = ksig.kernels.SignatureKernel(n_levels=n_levels, order=order, static_kernel=static_kernel)\n",
    "\n",
    "# Generate 10 sequences of length 50 with 5 channels.\n",
    "n_seq, l_seq, n_feat = 10, 50, 5 \n",
    "X = np.random.randn(n_seq, l_seq, n_feat)\n",
    "\n",
    "# Sequence kernels take as input an array of sequences of ndim == 3,\n",
    "# and work as a callable for computing the kernel matrix. \n",
    "K_XX = sig_kernel(X)  # K_XX has shape (10, 10).\n",
    "\n",
    "# The diagonal kernel entries can also be computed.\n",
    "K_X = sig_kernel(X, diag=True)  # K_X has shape (10,).\n",
    "\n",
    "# Generate another array of 8 sequences of length 20 and 5 features.\n",
    "n_seq2, l_seq2 = 8, 20\n",
    "Y = np.random.randn(n_seq2, l_seq2, n_feat)\n",
    "\n",
    "# Compute the kernel matrix between arrays X and Y.\n",
    "K_XY = sig_kernel(X, Y)  # K_XY has shape (10, 8)\n",
    "K_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test that iisig gives the same result as mine\n",
    "import iisignature\n",
    "import numpy as np\n",
    "import ksig\n",
    "\n",
    "# Number of signature levels to use.\n",
    "normalize=False\n",
    "trunc_level = 5\n",
    "geo_order = 5\n",
    "N=2\n",
    "N2= 2\n",
    "T, d = 20, 2\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(N, T, d, dtype=torch.float64).to(\"cuda\") / d\n",
    "Y = torch.randn(N2, T, d, dtype=torch.float64).to(\"cuda\") / d\n",
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "\n",
    "ksigker = ksig.kernels.SignatureKernel(static_kernel=ksig.static.kernels.LinearKernel(), \n",
    "                                       normalize=normalize,\n",
    "                                       n_levels=trunc_level, \n",
    "                                       order=geo_order)\n",
    "mine = TruncSigKernel(LinearKernel(scale=1), \n",
    "                      normalize=normalize, \n",
    "                      trunc_level=trunc_level, \n",
    "                      geo_order=geo_order, \n",
    "                      max_batch=50000)\n",
    "\n",
    "#test\n",
    "out1 = ksigker(X_np, Y_np)\n",
    "out2 = mine(X, Y)\n",
    "featuresX = iisignature.sig(X_np, trunc_level)\n",
    "featuresY = iisignature.sig(Y_np, trunc_level)\n",
    "out3 = 1+np.dot(featuresX, featuresY.T)\n",
    "print(\"ksig\", out1)\n",
    "print(\"\\nmine\", out2)\n",
    "print(\"\\niisig\", out3)\n",
    "print(np.mean(np.abs(out1 - out2.cpu().numpy())))\n",
    "print(np.mean(np.abs(out1 - out3)))\n",
    "print(np.mean(np.abs(out2.cpu().numpy() - out3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
