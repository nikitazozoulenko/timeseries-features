{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Any, Optional, Tuple, Literal, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import sigkernel\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from kernels.static_kernels import StaticKernel, AbstractKernel, RBFKernel\n",
    "from kernels.integral_type import IntegralKernel\n",
    "from kernels.sig_pde import SigPDEKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Any, Optional, Tuple, Literal, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import sigkernel\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "from kernels.static_kernels import StaticKernel, AbstractKernel, RBFKernel\n",
    "\n",
    "\n",
    "class TruncSigKernel(AbstractKernel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            static_kernel:StaticKernel = RBFKernel,\n",
    "            trunc_level:int = 5,\n",
    "            geo_order:int = 1,\n",
    "            only_last:bool = True,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        The truncated signature kernel of two time series of \n",
    "        shape (T_i, d) with respect to a static kernel on R^d.\n",
    "        See https://jmlr.org/papers/v20/16-314.html.\n",
    "\n",
    "        Args:\n",
    "            static_kernel (StaticKernel): Static kernel on R^d.\n",
    "            trunc_level (int): Truncation level of the signature kernel.\n",
    "            geo_order (int): Geometric order of the rough path lift.\n",
    "            only_last (bool): If False, returns results of all truncation \n",
    "                levels up to 'trunc_level'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.static_kernel = static_kernel\n",
    "        self.trunc_level = trunc_level\n",
    "        self.geo_order = geo_order\n",
    "        self.only_last = only_last\n",
    "\n",
    "\n",
    "    def gram(\n",
    "            self, \n",
    "            X: Tensor, \n",
    "            Y: Tensor, \n",
    "            diag: bool = False, \n",
    "        ):\n",
    "        \"\"\"\n",
    "        Computes the Gram matrix K(X_i, Y_j), or the diagonal K(X_i, Y_i) \n",
    "        if diag=True. The time series in X and Y are assumed to be of shape \n",
    "        (T1, d) and (T2, d) respectively. O(T^2(d + trunc_level*geo_order^2)) \n",
    "        time for each pair of time series.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Tensor with shape (N1, ..., T1, d).\n",
    "            Y (Tensor): Tensor with shape (N2, ..., T2, d), with (...) same as X.\n",
    "            diag (bool, optional): If True, only computes the kernel for the \n",
    "                pairs K(X_i, Y_i). Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Tensor with shape (N1, N2, ...), or (N1, ...) if diag=True.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "            self, \n",
    "            X: Tensor, \n",
    "            Y: Tensor, \n",
    "        )->Tensor:\n",
    "        \"\"\"\n",
    "        Computes the kernel evaluation k(X, Y) of two time series\n",
    "        (with batch support).\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Tensor with shape (... , T, d).\n",
    "            Y (Tensor): Tensor with shape (... , T, d), with (...) same as X.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Tensor with shape (...).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "#TODO: implement the 'gram' and '__call__' methods with a specified batch size.\n",
    "#TODO: THEN implement a method that loops through the batch size and computes the kernel.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sig_kernel(s1:Tensor, \n",
    "               s2:Tensor, \n",
    "               order:int,\n",
    "               static_kernel_gram:Callable = linear_kernel_gram,\n",
    "               only_last:bool = True):\n",
    "    \"\"\"Computes the truncated signature kernel of two time series of \n",
    "    shape (T_i, d) with respect to a static kernel on R^d.\n",
    "\n",
    "    Args:\n",
    "        s1 (np.ndarray): Array of shape (T_1, d).\n",
    "        s2 (np.ndarray): Array of shape (T_2, d).\n",
    "        order (int): Truncation order of the signature kernel.\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays,\n",
    "                            see e.g. 'linear_kernel_gram' or 'rbf_kernel_gram'.\n",
    "        only_last (bool): If False, returns results of all truncation levels up to 'order'.\n",
    "    \"\"\"\n",
    "    K = static_kernel_gram(s1, s2)\n",
    "    nabla = K[1:, 1:] + K[:-1, :-1] - K[1:, :-1] - K[:-1, 1:]\n",
    "    sig_kers = jitted_trunc_sig_kernel(nabla, order)\n",
    "    if only_last:\n",
    "        return sig_kers[-1]\n",
    "    else:\n",
    "        return sig_kers\n",
    "\n",
    "\n",
    "\n",
    "@njit((nb.float64[:, ::1], nb.int64), fastmath=True, cache=True)\n",
    "def reverse_cumsum(arr:Tensor, axis:int): #ndim=2\n",
    "    \"\"\"JITed reverse cumulative sum along the specified axis.\n",
    "    (np.cumsum with axis is not natively supported by Numba)\n",
    "    \n",
    "    Args:\n",
    "        arr (np.ndarray): Array of shape (T_1, T_2).\n",
    "        axis (int): Axis along which to cumsum.\n",
    "    \"\"\"\n",
    "    A = arr.copy()\n",
    "    if axis==0:\n",
    "        for i in np.arange(A.shape[0]-2, -1, -1):\n",
    "            A[i, :] += A[i+1, :]\n",
    "    else: #axis==1\n",
    "        for i in np.arange(A.shape[1]-2, -1, -1):\n",
    "            A[:,i] += A[:,i+1]\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "@njit((nb.float64[:, ::1], nb.int64), fastmath=True, cache=True)\n",
    "def jitted_trunc_sig_kernel(nabla, order):\n",
    "    \"\"\"Given difference matrix nabla_ij = K[i+1, j+1] + K[i, j] - K[i+1, j] - K[i, j+1],\n",
    "    computes the truncated signature kernel of all orders up to 'order'.\"\"\"\n",
    "    B = np.ones((order+1, order+1, order+1, *nabla.shape))\n",
    "    for d in np.arange(order):\n",
    "        for n in np.arange(order-d):\n",
    "            for m in np.arange(order-d):\n",
    "                B[d+1,n,m] = 1 + nabla/(n+1)/(m+1)*B[d, n+1, m+1]\n",
    "                r1 = reverse_cumsum(nabla * B[d, n+1, 1] / (n+1), axis=0)\n",
    "                B[d+1,n,m, :-1, :] += r1[1:, :]\n",
    "                r2 = reverse_cumsum(nabla * B[d, 1, m+1] / (m+1), axis=1)\n",
    "                B[d+1,n,m, :, :-1] += r2[:, 1:]\n",
    "                rr = reverse_cumsum(nabla * B[d, 1, 1], axis=0)\n",
    "                rr = reverse_cumsum(rr, axis=1)\n",
    "                B[d+1,n,m, :-1, :-1] += rr[1:, 1:]\n",
    "\n",
    "    #copy, otherwise all memory accumulates in for loop\n",
    "    return B[1:,0,0,0,0].copy() \n",
    "\n",
    "\n",
    "\n",
    "def sig_kernel_gram(\n",
    "        X:List[np.ndarray],\n",
    "        Y:List[np.ndarray],\n",
    "        order:int,\n",
    "        static_kernel_gram:Callable,\n",
    "        only_last:bool = True,\n",
    "        sym:bool = False,\n",
    "        n_jobs:int = 1,\n",
    "        verbose:bool = False,\n",
    "    ):\n",
    "    \"\"\"Computes the Gram matrix k_sig(X_i, Y_j) of the signature kernel,\n",
    "    given the static kernel k(x, y) and the truncation order.\n",
    "\n",
    "    Args:\n",
    "        X (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        Y (List[np.ndarray]): List of time series of shape (T_j, d).\n",
    "        order (int): Truncation level of the signature kernel.\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays,\n",
    "                            see e.g. 'linear_kernel_gram' or 'rbf_kernel_gram'.\n",
    "        only_last (bool): If False, returns results of all truncation levels up to 'order'.\n",
    "        sym (bool): If True, computes the symmetric Gram matrix.\n",
    "        n_jobs (int): Number of parallel jobs to run.\n",
    "        verbose (bool): Whether to enable the tqdm progress bar.\n",
    "    \"\"\"\n",
    "    pairwise_ker = lambda s1, s2 : sig_kernel(s1, s2, order, static_kernel_gram, only_last)\n",
    "    return pairwise_kernel_gram(X, Y, pairwise_ker, sym, n_jobs, verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kernel(kernel: AbstractKernel):\n",
    "    print(\"Testing\", kernel)\n",
    "\n",
    "    gram = kernel.gram(X, Y)\n",
    "    diag = kernel.gram(X, Z, diag=True)\n",
    "    batch_call = kernel(X, Z)\n",
    "    call = kernel(X[0], Z[0])\n",
    "    print(\"gram\", gram.shape)\n",
    "    print(\"diag\", diag.shape)\n",
    "    print(\"batch_call\", batch_call.shape)\n",
    "    print(\"call\", call.shape, call)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "N1 = 5\n",
    "N2 = 6\n",
    "T = 7\n",
    "d = 3\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand((N1, T, d), dtype=torch.float64)\n",
    "Y = torch.rand((N2, T, d), dtype=torch.float64)\n",
    "Z = torch.rand((N1, T, d), dtype=torch.float64)\n",
    "print(\"X\", X.shape)\n",
    "print(\"Y\", Y.shape)\n",
    "print(\"Z\", Z.shape)\n",
    "\n",
    "rbf = RBFKernel()\n",
    "integral = IntegralKernel(rbf)\n",
    "sigpde = SigPDEKernel(rbf)\n",
    "test_kernel(integral)\n",
    "test_kernel(sigpde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19095907, 0.16398031, 0.18620992, 0.19087091, 0.18484457,\n",
       "        0.20070106, 0.18909773, 0.15912799],\n",
       "       [0.20404385, 0.18593345, 0.20341185, 0.19370773, 0.19144636,\n",
       "        0.19042903, 0.15015761, 0.18421433],\n",
       "       [0.17757097, 0.22375255, 0.2031727 , 0.20130245, 0.21693933,\n",
       "        0.215425  , 0.21906583, 0.19302871],\n",
       "       [0.17806956, 0.15464444, 0.17315351, 0.22786706, 0.16353346,\n",
       "        0.21591671, 0.20266778, 0.16649882],\n",
       "       [0.19081092, 0.17317917, 0.18431613, 0.1979861 , 0.18953143,\n",
       "        0.20137106, 0.19320287, 0.18909152],\n",
       "       [0.18309522, 0.20545965, 0.19266658, 0.21450548, 0.19720925,\n",
       "        0.23623127, 0.23267491, 0.20573147],\n",
       "       [0.19311771, 0.17054372, 0.17832147, 0.2133925 , 0.1821493 ,\n",
       "        0.17488457, 0.19314751, 0.23912757],\n",
       "       [0.1866358 , 0.19747523, 0.19460503, 0.19089762, 0.18527863,\n",
       "        0.21100696, 0.19634381, 0.17301965],\n",
       "       [0.18869749, 0.17039173, 0.16953859, 0.21138455, 0.1998335 ,\n",
       "        0.20260233, 0.19730641, 0.17540512],\n",
       "       [0.18040173, 0.1975397 , 0.19636093, 0.19377798, 0.19503508,\n",
       "        0.20469484, 0.19612561, 0.17247707]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ksig\n",
    "\n",
    "# Number of signature levels to use.\n",
    "n_levels = 5 \n",
    "\n",
    "# Use the RBF kernel for vector-valued data as static (base) kernel.\n",
    "static_kernel = ksig.static.kernels.RBFKernel() \n",
    "\n",
    "# Instantiate the signature kernel, which takes as input the static kernel.\n",
    "n_levels = 5\n",
    "order = 1\n",
    "sig_kernel = ksig.kernels.SignatureKernel(n_levels=n_levels, order=order, static_kernel=static_kernel)\n",
    "\n",
    "# Generate 10 sequences of length 50 with 5 channels.\n",
    "n_seq, l_seq, n_feat = 10, 50, 5 \n",
    "X = np.random.randn(n_seq, l_seq, n_feat)\n",
    "\n",
    "# Sequence kernels take as input an array of sequences of ndim == 3,\n",
    "# and work as a callable for computing the kernel matrix. \n",
    "K_XX = sig_kernel(X)  # K_XX has shape (10, 10).\n",
    "\n",
    "# The diagonal kernel entries can also be computed.\n",
    "K_X = sig_kernel(X, diag=True)  # K_X has shape (10,).\n",
    "\n",
    "# Generate another array of 8 sequences of length 20 and 5 features.\n",
    "n_seq2, l_seq2 = 8, 20\n",
    "Y = np.random.randn(n_seq2, l_seq2, n_feat)\n",
    "\n",
    "# Compute the kernel matrix between arrays X and Y.\n",
    "K_XY = sig_kernel(X, Y)  # K_XY has shape (10, 8)\n",
    "K_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_kernel(s1:Tensor, \n",
    "               s2:Tensor, \n",
    "               order:int,\n",
    "               static_kernel_gram:Callable = linear_kernel_gram,\n",
    "               only_last:bool = True):\n",
    "    \"\"\"Computes the truncated signature kernel of two time series of \n",
    "    shape (T_i, d) with respect to a static kernel on R^d.\n",
    "\n",
    "    Args:\n",
    "        s1 (np.ndarray): Array of shape (T_1, d).\n",
    "        s2 (np.ndarray): Array of shape (T_2, d).\n",
    "        order (int): Truncation order of the signature kernel.\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays,\n",
    "                            see e.g. 'linear_kernel_gram' or 'rbf_kernel_gram'.\n",
    "        only_last (bool): If False, returns results of all truncation levels up to 'order'.\n",
    "    \"\"\"\n",
    "    K = static_kernel_gram(s1, s2)\n",
    "    nabla = K[1:, 1:] + K[:-1, :-1] - K[1:, :-1] - K[:-1, 1:]\n",
    "    sig_kers = jitted_trunc_sig_kernel(nabla, order)\n",
    "    if only_last:\n",
    "        return sig_kers[-1]\n",
    "    else:\n",
    "        return sig_kers\n",
    "\n",
    "\n",
    "\n",
    "@njit((nb.float64[:, ::1], nb.int64), fastmath=True, cache=True)\n",
    "def reverse_cumsum(arr:Tensor, axis:int): #ndim=2\n",
    "    \"\"\"JITed reverse cumulative sum along the specified axis.\n",
    "    (np.cumsum with axis is not natively supported by Numba)\n",
    "    \n",
    "    Args:\n",
    "        arr (np.ndarray): Array of shape (T_1, T_2).\n",
    "        axis (int): Axis along which to cumsum.\n",
    "    \"\"\"\n",
    "    A = arr.copy()\n",
    "    if axis==0:\n",
    "        for i in np.arange(A.shape[0]-2, -1, -1):\n",
    "            A[i, :] += A[i+1, :]\n",
    "    else: #axis==1\n",
    "        for i in np.arange(A.shape[1]-2, -1, -1):\n",
    "            A[:,i] += A[:,i+1]\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "@njit((nb.float64[:, ::1], nb.int64), fastmath=True, cache=True)\n",
    "def jitted_trunc_sig_kernel(nabla, order):\n",
    "    \"\"\"Given difference matrix nabla_ij = K[i+1, j+1] + K[i, j] - K[i+1, j] - K[i, j+1],\n",
    "    computes the truncated signature kernel of all orders up to 'order'.\"\"\"\n",
    "    B = np.ones((order+1, order+1, order+1, *nabla.shape))\n",
    "    for d in np.arange(order):\n",
    "        for n in np.arange(order-d):\n",
    "            for m in np.arange(order-d):\n",
    "                B[d+1,n,m] = 1 + nabla/(n+1)/(m+1)*B[d, n+1, m+1]\n",
    "                r1 = reverse_cumsum(nabla * B[d, n+1, 1] / (n+1), axis=0)\n",
    "                B[d+1,n,m, :-1, :] += r1[1:, :]\n",
    "                r2 = reverse_cumsum(nabla * B[d, 1, m+1] / (m+1), axis=1)\n",
    "                B[d+1,n,m, :, :-1] += r2[:, 1:]\n",
    "                rr = reverse_cumsum(nabla * B[d, 1, 1], axis=0)\n",
    "                rr = reverse_cumsum(rr, axis=1)\n",
    "                B[d+1,n,m, :-1, :-1] += rr[1:, 1:]\n",
    "\n",
    "    #copy, otherwise all memory accumulates in for loop\n",
    "    return B[1:,0,0,0,0].copy() \n",
    "\n",
    "\n",
    "\n",
    "def sig_kernel_gram(\n",
    "        X:List[np.ndarray],\n",
    "        Y:List[np.ndarray],\n",
    "        order:int,\n",
    "        static_kernel_gram:Callable,\n",
    "        only_last:bool = True,\n",
    "        sym:bool = False,\n",
    "        n_jobs:int = 1,\n",
    "        verbose:bool = False,\n",
    "    ):\n",
    "    \"\"\"Computes the Gram matrix k_sig(X_i, Y_j) of the signature kernel,\n",
    "    given the static kernel k(x, y) and the truncation order.\n",
    "\n",
    "    Args:\n",
    "        X (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        Y (List[np.ndarray]): List of time series of shape (T_j, d).\n",
    "        order (int): Truncation level of the signature kernel.\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays,\n",
    "                            see e.g. 'linear_kernel_gram' or 'rbf_kernel_gram'.\n",
    "        only_last (bool): If False, returns results of all truncation levels up to 'order'.\n",
    "        sym (bool): If True, computes the symmetric Gram matrix.\n",
    "        n_jobs (int): Number of parallel jobs to run.\n",
    "        verbose (bool): Whether to enable the tqdm progress bar.\n",
    "    \"\"\"\n",
    "    pairwise_ker = lambda s1, s2 : sig_kernel(s1, s2, order, static_kernel_gram, only_last)\n",
    "    return pairwise_kernel_gram(X, Y, pairwise_ker, sym, n_jobs, verbose)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
