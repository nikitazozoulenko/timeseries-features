{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Any, Optional, Tuple, Literal, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import sigkernel\n",
    "\n",
    "from kernels.static_kernels import StaticKernel, AbstractKernel\n",
    "from kernels.static_kernels import LinearKernel, RBFKernel, PolyKernel\n",
    "\n",
    "\n",
    "class CrisStaticWrapper:\n",
    "    def __init__(\n",
    "            self, \n",
    "            kernel: StaticKernel,\n",
    "        ):\n",
    "        \"\"\"Wrapper for static kernels for Cris Salvi's sigkernel library\"\"\"\n",
    "        self.kernel = kernel\n",
    "\n",
    "\n",
    "    def batch_kernel(\n",
    "            self, \n",
    "            X:Tensor, \n",
    "            Y:Tensor\n",
    "        ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Outputs k(X^i_t, Y^j_t)\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Tensor of shape (N, T1, d)\n",
    "            Y (Tensor): Tensor of shape (N, T2, d)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Tensor of shape (N, T1, T2)\n",
    "        \"\"\"\n",
    "        X = X.transpose(1,0)\n",
    "        Y = Y.transpose(1,0)\n",
    "        trans_gram = self.kernel.gram(X, Y) # shape (T1, T2, N)\n",
    "        return trans_gram.permute(2, 0, 1)\n",
    "\n",
    "\n",
    "    def Gram_matrix(\n",
    "            self, \n",
    "            X: Tensor, \n",
    "            Y: Tensor\n",
    "        ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Outputs k(X^i_s, Y^j_t)\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): Tensor of shape (N1, T1, d)\n",
    "            Y (Tensor): Tensor of shape (N2, T2, d)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Tensor of shape (N1, N2, T1, T2)\n",
    "        \"\"\"\n",
    "        N1, T1, d = X.shape\n",
    "        N2, T2, d = Y.shape\n",
    "        X = X.reshape(-1, d)\n",
    "        Y = Y.reshape(-1, d)\n",
    "        flat_gram = self.kernel.gram(X, Y) # shape (N1 * T1, N2 * T2)\n",
    "        gram = flat_gram.reshape(N1, T1, N2, T2)\n",
    "        return gram.permute(0, 2, 1, 3)\n",
    "    \n",
    "    \n",
    "\n",
    "class SigPDEKernel(AbstractKernel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            static_kernel: StaticKernel = RBFKernel(),\n",
    "            dyadic_order:int = 1,\n",
    "            max_batch:int = 10,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Signature PDE kernel for timeseries (x_1, ..., x_T) in R^d,\n",
    "        kernelized with a static kernel k : R^d x R^d -> R.\n",
    "\n",
    "        Args:\n",
    "            static_kernel (StaticKernel): Static kernel on R^d.\n",
    "            dyadic_order (int, optional): Dyadic order in PDE solver. Defaults to 1.\n",
    "            max_batch (int, optional): Max batch size for computations. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.static_wrapper = CrisStaticWrapper(static_kernel)\n",
    "        self.dyadic_order = dyadic_order\n",
    "        self.sig_ker = sigkernel.SigKernel(self.static_wrapper, dyadic_order)\n",
    "        self.max_batch = max_batch\n",
    "\n",
    "\n",
    "    def gram(\n",
    "            self, \n",
    "            X: Tensor, \n",
    "            Y: Tensor, \n",
    "            diag: bool = False, \n",
    "        ):\n",
    "        \"\"\"\n",
    "        Computes the Gram matrix K(X_i, Y_j), or the diagonal K(X_i, Y_i) \n",
    "        if diag=True. The time series in X are of shape (T1, d), and the\n",
    "        time series in Y are of shape (T2, d), where d is the path dimension.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Tensor with shape (N1, T1, d).\n",
    "            Y (Tensor): Tensor with shape (N2, T2, d).\n",
    "            diag (bool, optional): If True, only computes the kernel for the \n",
    "                pairs K(X_i, Y_i). Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Tensor with shape (N1, N2), or (N1) if diag=True.\n",
    "        \"\"\"\n",
    "        if diag:\n",
    "            return self.sig_ker.compute_kernel(X, Y, self.max_batch)\n",
    "        else:\n",
    "            return self.sig_ker.compute_Gram(X, Y, sym=(X is Y), max_batch=self.max_batch)\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "            self, \n",
    "            X: Tensor, \n",
    "            Y: Tensor, \n",
    "        )->Tensor:\n",
    "        \"\"\"\n",
    "        Computes the kernel evaluation k(X, Y) of two time series \n",
    "        (with batch support). The time series in X are of shape (T1, d), \n",
    "        and the time series in Y are of shape (T2, d), where d is the \n",
    "        path dimension.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Tensor with shape (... , T1, d).\n",
    "            Y (Tensor): Tensor with shape (... , T2, d), with (...) same as X.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Tensor with shape (...).\n",
    "        \"\"\"\n",
    "        if X.ndim == 2 and Y.ndim == 2:\n",
    "            X = X.unsqueeze(0)\n",
    "            Y = Y.unsqueeze(0)\n",
    "        return self.sig_ker.compute_kernel(X, Y, self.max_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([5, 7, 3])\n",
      "Y torch.Size([5, 8, 3])\n",
      "Z torch.Size([6, 7, 3])\n",
      "K torch.Size([5])\n",
      "G torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Playing around with sigkernel\n",
    "###########################################\n",
    "\n",
    "\n",
    "# Specify the static kernel (for linear kernel use sigkernel.LinearKernel())\n",
    "static_kernel = sigkernel.RBFKernel(sigma=0.5)\n",
    "\n",
    "# Specify dyadic order for PDE solver (int > 0, default 0, the higher the more accurate but slower)\n",
    "dyadic_order = 3\n",
    "\n",
    "# Specify maximum batch size of computation; if memory is a concern try reducing max_batch, default=100\n",
    "max_batch = 11\n",
    "\n",
    "# Initialize the corresponding signature kernel\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order)\n",
    "\n",
    "# Synthetic data\n",
    "batch = 5\n",
    "batch_z = 6\n",
    "len_x = 7\n",
    "len_y = 8\n",
    "dim = 3\n",
    "device = \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand((batch,len_x,dim), dtype=torch.float64, device=device) # shape (batch,len_x,dim)\n",
    "Y = torch.rand((batch,len_y,dim), dtype=torch.float64, device=device) # shape (batch,len_y,dim)\n",
    "Z = torch.rand((batch_z,len_x,dim), dtype=torch.float64, device=device) # shape (batch,len_y,dim)\n",
    "\n",
    "# Compute signature kernel \"batch-wise\" (i.e. k(x_1,y_1),...,k(x_batch, y_batch))\n",
    "K = signature_kernel.compute_kernel(X,Y,max_batch)\n",
    "\n",
    "# Compute signature kernel Gram matrix (i.e. k(x_i,y_j) for i,j=1,...,batch), also works for different batch_x != batch_y)\n",
    "G = signature_kernel.compute_Gram(X,X,sym=True, max_batch=max_batch)\n",
    "\n",
    "print(\"X\", X.shape)\n",
    "print(\"Y\", Y.shape)\n",
    "print(\"Z\", Z.shape)\n",
    "print(\"K\", K.shape)\n",
    "print(\"G\", G.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([5, 3])\n",
      "Y torch.Size([6, 3])\n",
      "Z torch.Size([5, 3])\n",
      "Testing <kernels.static_kernels.LinearKernel object at 0x7ffbe06b4210>\n",
      "gram torch.Size([5, 6])\n",
      "diag torch.Size([5])\n",
      "batch_call torch.Size([5])\n",
      "call torch.Size([1]) tensor([0.2525])\n",
      "\n",
      "\n",
      "Testing <kernels.static_kernels.RBFKernel object at 0x7ffae0ae53d0>\n",
      "gram torch.Size([5, 6])\n",
      "diag torch.Size([5])\n",
      "batch_call torch.Size([5])\n",
      "call torch.Size([1]) tensor([0.6467])\n",
      "\n",
      "\n",
      "Testing <kernels.static_kernels.PolyKernel object at 0x7ffae0ae4f50>\n",
      "gram torch.Size([5, 6])\n",
      "diag torch.Size([5])\n",
      "batch_call torch.Size([5])\n",
      "call torch.Size([1]) tensor([1.5687])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from kernels.static_kernels import StaticKernel, AbstractKernel\n",
    "from kernels.static_kernels import LinearKernel, RBFKernel, PolyKernel\n",
    "from kernels.integral_type import IntegralKernel\n",
    "\n",
    "# Start with static kernel experiments\n",
    "\n",
    "N1 = 5\n",
    "N2 = 6\n",
    "d = 3\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand((N1, d))\n",
    "Y = torch.rand((N2, d))\n",
    "Z = torch.rand((N1, d))\n",
    "print(\"X\", X.shape)\n",
    "print(\"Y\", Y.shape)\n",
    "print(\"Z\", Z.shape)\n",
    "\n",
    "def test_kernel(kernel: AbstractKernel):\n",
    "    print(\"Testing\", kernel)\n",
    "\n",
    "    gram = kernel.gram(X, Y)\n",
    "    diag = kernel.gram(X, Z, diag=True)\n",
    "    batch_call = kernel(X, Z)\n",
    "    call = kernel(X[0], Z[0])\n",
    "    print(\"gram\", gram.shape)\n",
    "    print(\"diag\", diag.shape)\n",
    "    print(\"batch_call\", batch_call.shape)\n",
    "    print(\"call\", call.shape, call)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "linear = LinearKernel()\n",
    "rbf = RBFKernel()\n",
    "poly = PolyKernel()\n",
    "test_kernel(linear)\n",
    "test_kernel(rbf)\n",
    "test_kernel(poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([5, 7, 3])\n",
      "Y torch.Size([6, 7, 3])\n",
      "Z torch.Size([5, 7, 3])\n",
      "Testing <kernels.integral_type.IntegralKernel object at 0x7ffbe5077bd0>\n",
      "gram torch.Size([5, 6])\n",
      "diag torch.Size([5])\n",
      "batch_call torch.Size([5])\n",
      "call torch.Size([1]) tensor([0.7511], dtype=torch.float64)\n",
      "\n",
      "\n",
      "Testing <__main__.SigPDEKernel object at 0x7ffae0b01f90>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m sigpde \u001b[38;5;241m=\u001b[39m SigPDEKernel(rbf)\n\u001b[1;32m     15\u001b[0m test_kernel(integral)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtest_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigpde\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mtest_kernel\u001b[0;34m(kernel)\u001b[0m\n\u001b[1;32m     22\u001b[0m diag \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mgram(X, Z, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m batch_call \u001b[38;5;241m=\u001b[39m kernel(X, Z)\n\u001b[0;32m---> 24\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgram\u001b[39m\u001b[38;5;124m\"\u001b[39m, gram\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiag\u001b[39m\u001b[38;5;124m\"\u001b[39m, diag\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m, in \u001b[0;36mSigPDEKernel.__call__\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    116\u001b[0m         X: Tensor, \n\u001b[1;32m    117\u001b[0m         Y: Tensor, \n\u001b[1;32m    118\u001b[0m     )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39mTensor:\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    Computes the kernel evaluation k(X, Y) of two time series \u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    (with batch support). The time series in X are of shape (T1, d), \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        Tensor: Tensor with shape (...).\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msig_ker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/timeseries-features/.conda/lib/python3.11/site-packages/sigkernel/sigkernel.py:31\u001b[0m, in \u001b[0;36mSigKernel.compute_kernel\u001b[0;34m(self, X, Y, max_batch)\u001b[0m\n\u001b[1;32m     29\u001b[0m batch \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_batch:\n\u001b[0;32m---> 31\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[43m_SigKernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdyadic_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_naive_solver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Code/timeseries-features/.conda/lib/python3.11/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/timeseries-features/.conda/lib/python3.11/site-packages/sigkernel/sigkernel.py:209\u001b[0m, in \u001b[0;36m_SigKernel.forward\u001b[0;34m(ctx, X, Y, static_kernel, dyadic_order, _naive_solver)\u001b[0m\n\u001b[1;32m    207\u001b[0m M \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    208\u001b[0m N \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 209\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    211\u001b[0m MM \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdyadic_order)\u001b[38;5;241m*\u001b[39m(M\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    212\u001b[0m NN \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdyadic_order)\u001b[38;5;241m*\u001b[39m(N\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "N1 = 5\n",
    "N2 = 6\n",
    "T = 7\n",
    "d = 3\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand((N1, T, d), dtype=torch.float64)\n",
    "Y = torch.rand((N2, T, d), dtype=torch.float64)\n",
    "Z = torch.rand((N1, T, d), dtype=torch.float64)\n",
    "print(\"X\", X.shape)\n",
    "print(\"Y\", Y.shape)\n",
    "print(\"Z\", Z.shape)\n",
    "\n",
    "integral = IntegralKernel(rbf)\n",
    "sigpde = SigPDEKernel(rbf)\n",
    "test_kernel(integral)\n",
    "test_kernel(sigpde)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
